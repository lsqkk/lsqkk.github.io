#!/usr/bin/env python3
"""
ä¸€é”®ç”Ÿæˆsitemap.xmlè„šæœ¬
æ³¨æ„ç”Ÿæˆåæ›¿æ¢å·¦å³æ–œæ 
"""

import os
import xml.etree.ElementTree as ET
from xml.dom import minidom
from datetime import datetime
from pathlib import Path
import webbrowser

def generate_sitemap():
    """
    ä¸€é”®ç”Ÿæˆsitemap.xml
    è‡ªåŠ¨æ£€æµ‹å½“å‰ç›®å½•ä½œä¸ºç½‘ç«™æ ¹ç›®å½•
    """
    
    # è‡ªåŠ¨è·å–å½“å‰ç›®å½•ä½œä¸ºç½‘ç«™æ ¹ç›®å½•
    root_dir = "D:\git\lsqkk\lsqkk.github.io"
    base_url = "https://lsqkk.github.io/"
    
    # é»˜è®¤æ’é™¤çš„ç›®å½•
    exclude_dirs = [
        '.git', '.vscode', '__pycache__', 'node_modules',
        '.idea', '.venv','venv', 'env', '.github', 'dist', 'build',
        'cache', '.svn', '.hg', 'test', 'tests', 'temp', 'rubbish', 'template'
    ]
    
    # é»˜è®¤æ’é™¤çš„æ–‡ä»¶
    exclude_files = [
        'sitemap.xml', 'robots.txt', '.gitignore', 'CNAME',
        'README.md', 'LICENSE', 'readme.md',  '404.html', 'auth.html'
    ]
    
    # åˆ›å»ºXMLæ ¹å…ƒç´ 
    urlset = ET.Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    # æ‰«ææ‰€æœ‰HTMLæ–‡ä»¶
    html_files = []
    
    for root, dirs, files in os.walk(root_dir):
        # æ’é™¤ä¸éœ€è¦çš„ç›®å½•
        dirs[:] = [d for d in dirs if d not in exclude_dirs]
        
        for file in files:
            if file.endswith('.html') or file.endswith('.htm'):
                # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶
                if file in exclude_files:
                    continue
                
                file_path = os.path.join(root, file)
                html_files.append(file_path)
    
    print(f"âœ… æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    print("-" * 40)
    
    if len(html_files) == 0:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•HTMLæ–‡ä»¶ï¼")
        print("è¯·ç¡®ä¿è„šæœ¬æ”¾åœ¨ç½‘ç«™æ ¹ç›®å½•ä¸‹ï¼Œä¸”ç½‘ç«™åŒ…å«HTMLæ–‡ä»¶")
        return
    
    # ä¸ºæ¯ä¸ªHTMLæ–‡ä»¶åˆ›å»ºURLæ¡ç›®
    url_count = 0
    
    for file_path in html_files:
        # è·å–ç›¸å¯¹è·¯å¾„
        rel_path = os.path.relpath(file_path, root_dir)
        
        # è®¡ç®—æ–‡ä»¶å¤¹æ·±åº¦ï¼ˆç”¨äºç¡®å®šä¼˜å…ˆçº§ï¼‰
        depth = rel_path.count(os.sep)
        
        # ä¼˜å…ˆçº§çš„è®¡ç®—ï¼šæ ¹ç›®å½•ä¸º1.0ï¼Œæ¯æ·±ä¸€çº§å‡å°‘0.1ï¼Œæœ€ä½0.1
        priority = max(1.0 - (depth * 0.1), 0.1)
        
        # å¯¹äºindex.htmlæ–‡ä»¶ï¼Œæ·±åº¦å‡1ï¼ˆå› ä¸ºindex.htmlé€šå¸¸ä»£è¡¨å½“å‰ç›®å½•ï¼‰
        file_name = os.path.basename(rel_path)
        if file_name == 'index.html' or file_name == 'index.htm':
            depth = max(0, depth - 1)
            priority = max(1.0 - (depth * 0.1), 0.1)
        
        # æ„å»ºå®Œæ•´URL
        # å¦‚æœæ˜¯index.htmlï¼Œä½¿ç”¨ç›®å½•è·¯å¾„
        if file_name == 'index.html' or file_name == 'index.htm':
            dir_path = os.path.dirname(rel_path)
            if dir_path == '.':
                url_path = '/'
            else:
                url_path = f'/{dir_path}/'
        else:
            # ç§»é™¤.htmlæˆ–.htmåç¼€ï¼Œåˆ›å»ºæ›´å‹å¥½çš„URL
            base_name = rel_path[:-5] if rel_path.endswith('.html') else rel_path[:-4]
            url_path = f'/{base_name}/'
        
        full_url = base_url.rstrip('/') + url_path
        
        # è·å–æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´
        try:
            lastmod = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d')
        except:
            lastmod = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ›å»ºURLå…ƒç´ 
        url_elem = ET.SubElement(urlset, 'url')
        
        # æ·»åŠ å­å…ƒç´ 
        loc = ET.SubElement(url_elem, 'loc')
        loc.text = full_url
        
        lastmod_elem = ET.SubElement(url_elem, 'lastmod')
        lastmod_elem.text = lastmod
        
        changefreq = ET.SubElement(url_elem, 'changefreq')
        changefreq.text = 'monthly'  # é»˜è®¤æ›´æ–°é¢‘ç‡
        
        priority_elem = ET.SubElement(url_elem, 'priority')
        priority_elem.text = f"{priority:.1f}"
        
        url_count += 1
        
        # æ˜¾ç¤ºè¿›åº¦
        if url_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªURL
            print(f"  {url_count:3d}. {full_url} (ä¼˜å…ˆçº§: {priority:.1f})")
        elif url_count == 4:
            print(f"  ... è¿˜æœ‰ {len(html_files) - 10} ä¸ªURLæœªæ˜¾ç¤º")
    
    # ç”ŸæˆXMLå­—ç¬¦ä¸²
    xml_string = ET.tostring(urlset, encoding='utf-8')
    
    # ç¾åŒ–XMLè¾“å‡º
    reparsed = minidom.parseString(xml_string)
    pretty_xml = reparsed.toprettyxml(indent='  ', encoding='utf-8')
    
    # å†™å…¥æ–‡ä»¶
    output_path = os.path.join(root_dir, 'sitemap.xml')
    with open(output_path, 'wb') as f:
        f.write(pretty_xml)
    
    print("-" * 30)
    print(f"sitemap.xml ç”ŸæˆæˆåŠŸï¼Œä½äº {output_path} ï¼ŒåŒ…å« {url_count} ä¸ªURL")

'''
def create_robots_txt():
    """åŒæ—¶åˆ›å»ºrobots.txtæ–‡ä»¶"""
    root_dir = get_current_directory()
    robots_path = os.path.join(root_dir, 'robots.txt')
    
    if not os.path.exists(robots_path):
        robots_content = f"""# robots.txt for {root_dir}
User-agent: *
Allow: /
Sitemap: /sitemap.xml

# Crawl-delay: 10
# Disallow: /private/
# Disallow: /tmp/
"""
        with open(robots_path, 'w', encoding='utf-8') as f:
            f.write(robots_content)
        print(f"ğŸ“„ å·²åˆ›å»º robots.txt æ–‡ä»¶")
    else:
        print(f"ğŸ“„ robots.txt æ–‡ä»¶å·²å­˜åœ¨")
'''

def main():
    """ä¸»å‡½æ•° - ç›´æ¥è¿è¡Œ"""
    try:
        # ç”Ÿæˆsitemap
        success = generate_sitemap()

        '''
        if success:
            # è¯¢é—®æ˜¯å¦åˆ›å»ºrobots.txt
            create_robots = input("\næ˜¯å¦åˆ›å»ºrobots.txtæ–‡ä»¶ï¼Ÿ(y/n): ").lower()
            if create_robots == 'y' or create_robots == 'yes':
                create_robots_txt()
            
            print("\n" + "ğŸ‰ å®Œæˆï¼".center(60))
        '''

    except Exception as e:
        print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        
        # ç­‰å¾…ç”¨æˆ·æŒ‰ä»»æ„é”®é€€å‡º
        input("\næŒ‰å›è½¦é”®é€€å‡º...")

if __name__ == '__main__':
    main()